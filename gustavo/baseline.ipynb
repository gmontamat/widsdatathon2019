{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "## Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), '..', 'input')\n",
    "labels = os.path.join(data_path, 'traininglabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>has_oilpalm</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_000002017.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_000012017.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_000022017.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_000072017.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_000082017.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id  has_oilpalm   score\n",
       "0  img_000002017.jpg            0  0.7895\n",
       "1  img_000012017.jpg            0  1.0000\n",
       "2  img_000022017.jpg            0  1.0000\n",
       "3  img_000072017.jpg            0  1.0000\n",
       "4  img_000082017.jpg            0  1.0000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_as_array(image_id, size=None, image_set='train_images'):\n",
    "    image_path = os.path.join(data_path, image_set, image_id)\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if size is None:\n",
    "        return img\n",
    "    return cv2.resize(img, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 256\n",
    "width = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "responses = []\n",
    "scores = []\n",
    "for idx, row in df.iterrows():\n",
    "    img = img_as_array(row[0])  #, size=(height, width))\n",
    "    images.append(img.reshape(1, height, width, 3))\n",
    "    responses.append(row[1])\n",
    "    scores.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate(images, axis=0)\n",
    "responses = np.array(responses).reshape(images.shape[0], 1)\n",
    "scores = np.array(scores).reshape(images.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responses: (15244, 1)\n",
      "scores: (15244, 1)\n",
      "images: (15244, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"responses: {}\".format(responses.shape))\n",
    "print(\"scores: {}\".format(scores.shape))\n",
    "print(\"images: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle train set for future usage\n",
    "\n",
    "*NOTE: Images aren't normalized!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'train_images_256x256.pkl'), 'wb') as fout:\n",
    "    pickle.dump(images, fout, protocol=4)\n",
    "with open(os.path.join(data_path, 'train_responses.pkl'), 'wb') as fout:\n",
    "    pickle.dump(responses, fout, protocol=4)\n",
    "with open(os.path.join(data_path, 'train_scores.pkl'), 'wb') as fout:\n",
    "    pickle.dump(scores, fout, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also generate pickles of resized images in case we need them for simpler models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images64 = []\n",
    "for idx, row in df.iterrows():\n",
    "    img = img_as_array(row[0], size=(64, 64))\n",
    "    images64.append(img.reshape(1, 64, 64, 3))\n",
    "images64 = np.concatenate(images64, axis=0)\n",
    "with open(os.path.join(data_path, 'train_images_64x64.pkl'), 'wb') as fout:\n",
    "    pickle.dump(images64, fout, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images128 = []\n",
    "for idx, row in df.iterrows():\n",
    "    img = img_as_array(row[0], size=(128, 128))\n",
    "    images128.append(img.reshape(1, 128, 128, 3))\n",
    "images128 = np.concatenate(images128, axis=0)\n",
    "with open(os.path.join(data_path, 'train_images_128x128.pkl'), 'wb') as fout:\n",
    "    pickle.dump(images128, fout, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resized images for now, we won't use them for the baseline\n",
    "del images64\n",
    "del images128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = images / 255.\n",
    "images = images * 2. / 255. - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications import vgg16\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_callback(Callback):\n",
    "    \"\"\"Define a callback which returns train ROC AUC after each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, training_data, validation_data=None):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        # self.x_val = validation_data[0]\n",
    "        # self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        # y_pred_val = self.model.predict(self.x_val)\n",
    "        # roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        # print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        print('\\rroc-auc: {}'.format(round(roc, 5)), end=80 * ' ' + '\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,780,481\n",
      "Trainable params: 14,780,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def baseline_vgg():\n",
    "    vgg = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(height, width, 3), pooling='max')\n",
    "    last = vgg.output\n",
    "    # x = Flatten()(last)\n",
    "    x = Dense(128, activation='relu')(last)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs=[vgg.input], outputs=[x])\n",
    "\n",
    "model = baseline_vgg()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15244/15244 [==============================] - 122s 8ms/step - loss: 0.0877 - acc: 0.9715\n",
      "roc-auc: 0.99515                                                                                \n",
      "Epoch 2/10\n",
      "15244/15244 [==============================] - 120s 8ms/step - loss: 0.0315 - acc: 0.9901\n",
      "roc-auc: 0.99863                                                                                \n",
      "Epoch 3/10\n",
      "15244/15244 [==============================] - 117s 8ms/step - loss: 0.0267 - acc: 0.9909\n",
      "roc-auc: 0.99911                                                                                \n",
      "Epoch 4/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0235 - acc: 0.9920\n",
      "roc-auc: 0.99921                                                                                \n",
      "Epoch 5/10\n",
      "15244/15244 [==============================] - 113s 7ms/step - loss: 0.0196 - acc: 0.9937\n",
      "roc-auc: 0.99948                                                                                \n",
      "Epoch 6/10\n",
      "15244/15244 [==============================] - 113s 7ms/step - loss: 0.0161 - acc: 0.9948\n",
      "roc-auc: 0.99944                                                                                \n",
      "Epoch 7/10\n",
      "15244/15244 [==============================] - 112s 7ms/step - loss: 0.0149 - acc: 0.9956\n",
      "roc-auc: 0.99971                                                                                \n",
      "Epoch 8/10\n",
      "15244/15244 [==============================] - 112s 7ms/step - loss: 0.0148 - acc: 0.9953\n",
      "roc-auc: 0.99972                                                                                \n",
      "Epoch 9/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0127 - acc: 0.9960\n",
      "roc-auc: 0.99982                                                                                \n",
      "Epoch 10/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0123 - acc: 0.9958\n",
      "roc-auc: 0.9995                                                                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faac32696a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "model.fit(images, responses, batch_size=16, epochs=10, callbacks=[roc_callback(training_data=(images, responses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('vgg_baseline_10epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15244/15244 [==============================] - 121s 8ms/step - loss: 0.0089 - acc: 0.9978\n",
      "roc-auc: 0.99986                                                                                \n",
      "Epoch 2/10\n",
      "15244/15244 [==============================] - 122s 8ms/step - loss: 0.0067 - acc: 0.9982\n",
      "roc-auc: 0.99987                                                                                \n",
      "Epoch 3/10\n",
      "15244/15244 [==============================] - 115s 8ms/step - loss: 0.0064 - acc: 0.9978\n",
      "roc-auc: 0.99987                                                                                \n",
      "Epoch 4/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0065 - acc: 0.9982\n",
      "roc-auc: 0.99988                                                                                \n",
      "Epoch 5/10\n",
      "15244/15244 [==============================] - 112s 7ms/step - loss: 0.0065 - acc: 0.9981\n",
      "roc-auc: 0.99989                                                                                \n",
      "Epoch 6/10\n",
      "15244/15244 [==============================] - 113s 7ms/step - loss: 0.0057 - acc: 0.9984\n",
      "roc-auc: 0.9999                                                                                \n",
      "Epoch 7/10\n",
      "15244/15244 [==============================] - 115s 8ms/step - loss: 0.0051 - acc: 0.9984\n",
      "roc-auc: 0.9999                                                                                \n",
      "Epoch 8/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0057 - acc: 0.9984\n",
      "roc-auc: 0.99991                                                                                \n",
      "Epoch 9/10\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0057 - acc: 0.9986\n",
      "roc-auc: 0.99991                                                                                \n",
      "Epoch 10/10\n",
      "15244/15244 [==============================] - 112s 7ms/step - loss: 0.0050 - acc: 0.9986\n",
      "roc-auc: 0.99992                                                                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4281b58d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-5, momentum=0.9), metrics=['accuracy'])\n",
    "model.fit(images, responses, batch_size=16, epochs=10, callbacks=[roc_callback(training_data=(images, responses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('vgg_baseline_20epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15244/15244 [==============================] - 116s 8ms/step - loss: 0.0053 - acc: 0.9985\n",
      "roc-auc: 0.99992                                                                                \n",
      "Epoch 2/5\n",
      "15244/15244 [==============================] - 115s 8ms/step - loss: 0.0050 - acc: 0.9986\n",
      "roc-auc: 0.99992                                                                                \n",
      "Epoch 3/5\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0050 - acc: 0.9990\n",
      "roc-auc: 0.99992                                                                                \n",
      "Epoch 4/5\n",
      "15244/15244 [==============================] - 114s 7ms/step - loss: 0.0044 - acc: 0.9991\n",
      "roc-auc: 0.99992                                                                                \n",
      "Epoch 5/5\n",
      "15244/15244 [==============================] - 112s 7ms/step - loss: 0.0050 - acc: 0.9986\n",
      "roc-auc: 0.99992                                                                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4281b59b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-6, momentum=0.9), metrics=['accuracy'])\n",
    "model.fit(images, responses, batch_size=16, epochs=5, callbacks=[roc_callback(training_data=(images, responses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('vgg_baseline_25epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'leaderboard_test_data'\n",
    "holdout_dir = 'leaderboard_holdout_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_ids = []\n",
    "for image_id in os.listdir(os.path.join(data_path, test_dir)):\n",
    "    img = img_as_array(image_id, image_set=test_dir)\n",
    "    test_images.append(img.reshape(1, height, width, 3))\n",
    "    test_ids.append(image_id)\n",
    "for image_id in os.listdir(os.path.join(data_path, holdout_dir)):\n",
    "    img = img_as_array(image_id, image_set=holdout_dir)\n",
    "    test_images.append(img.reshape(1, height, width, 3))\n",
    "    test_ids.append(image_id)\n",
    "test_images = np.concatenate(test_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images = test_images / 255.\n",
    "test_images = test_images * 2. / 255. - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.csv','w') as fout:\n",
    "    fout.write(\"image_id,has_oilpalm\\n\")\n",
    "    for image_id, has_oilpalm in zip(test_ids, predictions):\n",
    "        fout.write(\"{},{}\\n\".format(image_id, has_oilpalm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
